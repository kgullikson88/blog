<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Adventures of the Datastronomer</title><link href="http://kgullikson88.github.io/blog/" rel="alternate"></link><link href="http://kgullikson88.github.io/blog/feeds/visualization.atom.xml" rel="self"></link><id>http://kgullikson88.github.io/blog/</id><updated>2015-07-26T05:00:00-05:00</updated><entry><title>PyPi Dependency Analysis</title><link href="http://kgullikson88.github.io/blog/pypi-analysis.html" rel="alternate"></link><updated>2015-07-26T05:00:00-05:00</updated><author><name>Kevin Gullikson</name></author><id>tag:kgullikson88.github.io,2015-07-26:blog/pypi-analysis.html</id><summary type="html">&lt;p&gt;If you use the Python programming language, you have probably run the command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install &lt;span class="o"&gt;[&lt;/span&gt;package&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;at some point. What you may not know is the magic happening behind the scenes. The &lt;code&gt;pip&lt;/code&gt; command
is connecting to the &lt;a href="https://pypi.python.org/pypi"&gt;Pypi server&lt;/a&gt; and searching for the package you want. 
Once it finds that package, it downloads and runs a special python file titled &lt;code&gt;setup.py&lt;/code&gt;, which contains a 
bunch of metadata for the package. &lt;/p&gt;
&lt;p&gt;Knowing this, I decided to see what I could learn from the metadata available in the &lt;code&gt;setup.py&lt;/code&gt; file for 
every package on the Pypi server. There are a few things that are conceivable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parse all of the dependencies from every package. By dependencies I mean other python packages that the given package relies on.&lt;/li&gt;
&lt;li&gt;Parse the package description, and try to do something fun with it. Maybe I will write a &lt;a href="http://kgullikson88.github.io/blog/markov-chain.html"&gt;Markov chain text generator&lt;/a&gt; at some point to generate python package names and descriptions. Another more interesting thing would be to analyze the description with some natural language processing algorithm&lt;/li&gt;
&lt;li&gt;Tally up the version strings for each of the packages, and find weird ones/outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I might visit the other options in a later post, but here I will be looking at the dependencies. This post 
focuses on a programming language, and will necessarily be more technical than the other ones. Pretty 
pictures near the bottom.&lt;/p&gt;
&lt;h2&gt;Parsing Package Dependencies&lt;/h2&gt;
&lt;p&gt;Still here? Cool, let's get into how I did it. The first thing I needed to do was just figure out what the
dependencies of a given package are. That turned out to be way harder than it has any right to be. The 
command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip show &lt;span class="o"&gt;[&lt;/span&gt;package&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;gives a bunch of metadata for the given package, including everything it requires to run, but &lt;strong&gt;it only 
works if you have the package installed!&lt;/strong&gt; I am not about to install every package on pypi on my or anyone 
else's computer, so had to look for a more hacky way to do this. It turns out &lt;a href="https://ogirardot.wordpress.com/2013/01/05/state-of-the-pythonpypi-dependency-graph/"&gt;Olivier Girardot&lt;/a&gt; did a similar project 
a few years back, so I took their code as a starting point. &lt;/p&gt;
&lt;p&gt;The first thing I did was download every package on pypi, and extract the &lt;code&gt;setup.py&lt;/code&gt; file and any file or directory with the work 'requirement' in it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#TODO: Enter code to do this here.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first step left me with a bunch of directories that contain all the metadata for every pypi package (TODO: package and add this to Downloads...). Next, I used a slightly modified version of the &lt;a href="https://github.com/landscapeio/requirements-detector"&gt;requirements-detector&lt;/a&gt; package to parse out the requirements. The package does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search the &lt;code&gt;setup.py&lt;/code&gt; file for an &lt;code&gt;install_requires&lt;/code&gt; keyword, and attempt to parse package names out of that.&lt;/li&gt;
&lt;li&gt;If step one fails, search any file with the word 'requirement' in it, and look for things that look like python requirements&lt;/li&gt;
&lt;li&gt;Failing &lt;strong&gt;that&lt;/strong&gt;, search any file that ends with '.txt' in any directory that contains the word 'requirement' for stuff that looks like python requirements.&lt;/li&gt;
&lt;li&gt;Output the requirements found to a text file&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Of the [Ntotal] packages on pypi, I was able to parse requirements for [Nreq]. The remaining packages probably do require other packages, but the &lt;code&gt;setup.py&lt;/code&gt; file is written in such a way that it was difficult to parse. Sadly, my &lt;a href="https://pypi.python.org/pypi/TelFit/1.3.2"&gt;TelFit package&lt;/a&gt; is one of those failures because it defines the install_requires keyword programmatically. Leaving those out probably biases the result in some complex way, but I am sick of munging so let's move on to the fun part.&lt;/p&gt;
&lt;h2&gt;Dependency Analysis&lt;/h2&gt;
&lt;p&gt;Now that I have all of the dependencies for (most of) the packages on the pypi server, I want to see what I can learn. The first thing is to make a network graph of dependencies:&lt;/p&gt;
&lt;p&gt;TODO: Make picture of the graph that links to the javascript version...&lt;/p&gt;
&lt;p&gt;TODO: Describe the graph&lt;/p&gt;
&lt;p&gt;TODO: Figure out other interesting statistics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of the number of dependencies for each package.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Network_theory#Centrality_measures"&gt;Centrality measures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Compute the &lt;a href="https://en.wikipedia.org/wiki/PageRank"&gt;PageRank&lt;/a&gt; of each node, make a histogram similar to the first stat.&lt;/li&gt;
&lt;/ul&gt;</summary><category term="Python"></category><category term="web-scraping"></category></entry></feed>